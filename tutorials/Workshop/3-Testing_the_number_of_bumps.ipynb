{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260f0bcd-daa6-4876-8197-ed7067ce98d6",
   "metadata": {},
   "source": [
    "# Testing the number of events that best explains the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c2e2e-eaf4-43ec-a368-41f60db9ced9",
   "metadata": {},
   "source": [
    "We usually do not have precise enough information to decide a priori on a number of events that an HSM model should have. To illustrate the problem let's use the data from application 2 of this [this](https://link.springer.com/article/10.1007/s42113-021-00105-2) paper that we already prepared in section 1 of tutorial 1 and saved in ```sample_data/```. For the purpose of this tutorial we will only use the last four participants of the data (see [this](https://www.sciencedirect.com/science/article/pii/S1053811914002249) paper for the method and [https://osf.io/pd3kf/](https://osf.io/pd3kf/) for the whole (unpreprocessed) data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fa7d3",
   "metadata": {},
   "source": [
    "In this tutorial we will look at three different ways to discover the number of events in the data:\n",
    "- clustering\n",
    "- bootstrapping\n",
    "- leave-one-out-cross validation (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2c6c2-63a9-4ada-a99f-218a8c2ff633",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf03dd7-ac02-46f8-81e2-0a9fcd5bd9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import hsmm_mvpy as hmp\n",
    "from mne.io import read_info\n",
    "\n",
    "cpus=10#set the number of cores to use for all the code\n",
    "\n",
    "epoch_data = xr.load_dataset(os.path.join('sample_data/sample_data.nc'))\n",
    "#We create also the position array for the electrodes as we are going to need them to plot the event topologies\n",
    "info = read_info(os.path.join('sample_data/eeg/processed_0022_epo.fif'), verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc0ab5-b2be-4cf8-8a97-062fa93a31d9",
   "metadata": {},
   "source": [
    "First let's transform the data in principal component (PC) space, currently we have the epoched EEG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bef1c3-3ae5-45e5-91dc-90c54312338f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000cf2a-26f7-47d1-84be-3d6f10cab9b0",
   "metadata": {},
   "source": [
    "When transforming the data, a prompt request us how many PC we want to keep. The best is to maximise the number of components kept so that we don't loose PCs with useful information. However, the more PCs we include the more computationally intensive the estimations will be. For this two rules of thumb exists, taking PCs that explain x% of the variance of the data or select the PC number at which the explained variance seems to almost stop decreasing (i.e. the \"elbow\" method).\n",
    "\n",
    "The HMP `transform_data` function will show you the two graphs below, and ask you how many PCs you would like to retain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80e9eb-1e85-431a-9e73-1056376a37fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp_data = hmp.utils.transform_data(epoch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad137c-f47b-4f7f-8d54-69bc54c31f3c",
   "metadata": {},
   "source": [
    "In this case we see that 99% is achieved with 4 PCs and this is globally coherent with when the explained variance really drops (with a larger number of electrodes than the 30 used here, 99% might be achieved with a lot more PC but the elbow might happen before that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a72ee-821e-4dd4-872b-7b03eb8e1266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(hmp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11090f-4276-4e5e-bb7d-051be01ae57f",
   "metadata": {},
   "source": [
    "And we initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0879e-b78a-4083-a37b-9958b8e326ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init = hmp.models.hmp(hmp_data, epoch_data, event_width=50, cpus=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925efe9-74bd-42ab-bf20-f9563e0095cf",
   "metadata": {},
   "source": [
    "# Discovering a event number through a cumulative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c787865-b6eb-4b03-8067-586d433226e3",
   "metadata": {},
   "source": [
    "As introduced in Tutorial 2, the ```fit``` function works with starting points to the stage duration from 0 to mean RT. When an event is found (convergences of the expectation maximization algorithm) one event is added to the model and the slide continues. This way we can detect events while accounting for the previous ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dec483-b72f-4bbd-8dcb-cea9b5e6c48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimates = init.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddb7e3-3906-4566-a3fe-e16fdb1cce62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates, info, init, times_to_display=np.mean(init.ends - init.starts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac01f2-3178-4a31-9fca-ae9d731413f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T21:27:49.313865Z",
     "iopub.status.busy": "2023-05-03T21:27:49.313595Z",
     "iopub.status.idle": "2023-05-03T21:27:49.947312Z",
     "shell.execute_reply": "2023-05-03T21:27:49.946847Z",
     "shell.execute_reply.started": "2023-05-03T21:27:49.313849Z"
    },
    "tags": []
   },
   "source": [
    "### Illustration of the principle of bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf45e7-1ed9-4bd6-9a76-34eba9624078",
   "metadata": {},
   "source": [
    "In the case of EEG data for a group or even a single participant we can bootstrap the epochs or trials of the EEG. The idea is that 1) generating a bootstraped dataset, 2) fitting a model using the cumulative approach and 3) recording the parameters of the HMP model. We expect that with enough bootstrapped sampels we will know wich events tend to be rare (e.g. strategies) and what is the variance in time and electrode contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620f021-26fc-4b1f-ab35-17039429f468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrapped = hmp.resample.bootstrapping(data=epoch_data, dim=[ 'epochs','participant'], n_iterations=1, \n",
    "                                                 init=init, positions=info, sfreq=epoch_data.sfreq, cpus=cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd8f76-de77-4849-9531-00aaa2225781",
   "metadata": {},
   "source": [
    "Next we compare to the times that we estimated on the initial run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c370a1a-f88b-4498-abe9-23bb65aec067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "times_general_run = init.compute_times(init, estimates, duration=False, mean=True, add_rt=True)\n",
    "hmp.visu.plot_topo_timecourse(bootstrapped.sel(iteration=0).channels_activity.values, \n",
    "                              bootstrapped.sel(iteration=0).event_times.values,\n",
    "                              info, init, times_to_display=times_general_run.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06b4d2-0d1c-4e6e-b128-a7d2e00206b9",
   "metadata": {},
   "source": [
    "Of course this is automated in the HMP package, here an example for 10 bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9486690-0a2a-4242-8939-0598e361dfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrapped = hmp.resample.bootstrapping(data=epoch_data, dim=['epochs','participant'], n_iterations=10, \n",
    "                                                 init=init, positions=info, sfreq=epoch_data.sfreq, cpus=cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc6d8e-fec1-4072-8326-8c9642c251b0",
   "metadata": {},
   "source": [
    "We can then plot the bootstrapped models (or leave the plots argument as True when calling the previous function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d99c3-7c14-4c20-92bf-11a938da5c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(bootstrapped.channels_activity.values, \n",
    "                              bootstrapped.event_times.values,\n",
    "                              info, init, times_to_display=times_general_run.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204b5be-38aa-423a-bab9-8d42eb84da07",
   "metadata": {},
   "source": [
    "We can then use these bootstrapped data to 1) assess variability in event detection for a given threshold and 2) the variation in the location of the events and electrode contribution for a given _n_ event model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e770b2-e2ee-45f3-b7b6-3c9947fc4060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_bootstrap_results(bootstrapped, info, init,\n",
    "                               epoch_data = epoch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45e840-b061-458f-a3fb-8329510cab4f",
   "metadata": {},
   "source": [
    "The bootstrapped object contains all the bootstrapped run and we can use it to inspect different properties of the bootstrapped sample, e.g. the standard deviation of the scale parameter of the gamma from the last stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438090eb-e798-4908-ab60-07248f2ec1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrapped.sel(stage=3, parameter=\"scale\").event_times.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb02a3a-fe02-4bbc-b022-a5bc46454c50",
   "metadata": {},
   "source": [
    "## Fitting from the maximum number of events to a 1 event model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7b578-48a5-4a1d-b0c9-e08789484d07",
   "metadata": {},
   "source": [
    "An alternative way of assessing the number of events is to use the ```backward_estimation``` as discussed in the previous tutorial. In contrast to the previous tutorial, here we first calculate a fit for a model with the maximum number of fitting events using 100 random starting points, and then use that as starting estimates for the backward estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5e9ce-eef7-4932-a225-d0592d5649a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_fit = init.fit_single(init.compute_max_events())\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, max_fit, info, init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0455be9-6506-49c5-b3cc-297a4d2b7c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "bests = init.backward_estimation(max_fit=max_fit)#we can also leave the max_fit argument empty and it will be computed as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd43823-d832-41a0-b987-e5488bc89fd9",
   "metadata": {},
   "source": [
    "Here we plot the resulting solutions going from the maximal possible number of events that fit into the minimum RT given a duration of 50ms (default) in this example ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d9afb-ebfe-46a5-947a-47be089bb4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, bests, info, init, ydim='n_events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fe14a-9dfd-443e-b328-6e58af51f4b4",
   "metadata": {},
   "source": [
    "But adding more and more parameters will almost always improve the fit (except when events start pushing each other because of the defined minimum duration). To illustrate this we can plot the raw likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ae4bd-710f-4e17-b026-38fd256c545c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(bests.n_events, bests.likelihoods,'o-')\n",
    "\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.xlabel('N-event model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5842e-e879-43b9-bf4c-63a116c1fd06",
   "metadata": {},
   "source": [
    "Hence we need a way to penalize the likelihood if the events location do not generalize to all participants. One way to do this is to perform a leave-one out cross-validation. This approach consists in fitting the model to all participants but one, and evaluate the likelihood of the left out participant given the estimated parameters without the participant. The function ```hmp.utils.loocv_mp``` does this and we can apply it to all participants and all *n* event model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753aab9e-8a96-4031-9ff5-289ebb60a3c9",
   "metadata": {},
   "source": [
    "Note that the LOOCV can be very slow when there are a lot of participants, in this case it might be worth it to add multiprocessing through the cpus parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca145a55-31a5-4894-9368-cc122f51d730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "loocv = hmp.utils.loocv_mp(init, hmp_data, bests, cpus=4)#We set the cpus to 4 as there are 4 participants in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d30d7e-25c4-4ef9-a309-f63922451cb8",
   "metadata": {},
   "source": [
    "We can then plot the result from the LOOCV procedure, with the likelihood per participant in the graph on the left, and the changes in likelihood on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c42aa-7fc0-476f-8100-ba4a10cb572d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_loocv(loocv, pvals=True, test='sign', indiv=True)#Colored shaded lines represent individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a47b77-e832-4847-8216-7e03bccea887",
   "metadata": {},
   "source": [
    "With 4 participants the results are obviously not to be taken too seriously. The figure contains the p-value for a sign test evaluating whether the fit improved for a significant number of subjects, but it cannot be significant with 4 participants. In this case going 4 events is the best option (see right panel), considering that it has the most events for which a significant number of participants improved (4/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763257f-acc5-4a8e-bd3f-98e2b69b4ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = bests.sel(n_events=4)\n",
    "\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init,  figsize=(12,1),\n",
    "                                time_step = 1,  times_to_display = np.mean(init.ends - init.starts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d77ff-d7d7-49c5-acb8-b239d0b7758f",
   "metadata": {},
   "source": [
    "This selection method suggests one more event although the evidence is not large. With enough participants all event selection models should eventually converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5078b-a828-48f3-9e58-5d2b3bfb808a",
   "metadata": {},
   "source": [
    "# Using clustering on the sliding event function\n",
    "\n",
    "As seen in the previous tutorial we can also use several starting points on both the parameters and the magnitudes using the sliding event function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19161a-592a-4e77-a4d3-903ff8c75a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calc estimates\n",
    "lkhs, mags, channels, times = init.sliding_event_mags(epoch_data, step=3, decimate_grid = 3, cpu=cpus, plot=True, min_iteration=10)\n",
    "\n",
    "# cluster in time, lkh, and mags\n",
    "mags_cl, pars_cl = hmp.clusters.cluster_events(init, lkhs, mags, channels, times, method='time_x_lkh_x_mags', max_clust=10, p_outlier=.05, info=info, calc_outliers=True)\n",
    "\n",
    "#fit final model\n",
    "best_estimate = init.fit_single(mags_cl.shape[0], magnitudes=mags_cl,parameters=pars_cl)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, best_estimate, info, init, magnify=1, sensors=False, time_step=1000/init.sfreq,xlabel='Time (ms)', contours=0, event_lines=True, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3088cab-a96f-48a1-b0d1-e6af2f960586",
   "metadata": {},
   "source": [
    "This method is in agreement with the ```fit()``` method of the first section.\n",
    "\n",
    "After these selection procedure we can move to further analysis of the data and the HMP partition, see next tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
