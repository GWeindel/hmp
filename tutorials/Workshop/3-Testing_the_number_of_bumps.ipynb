{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260f0bcd-daa6-4876-8197-ed7067ce98d6",
   "metadata": {},
   "source": [
    "# Discovering the number of events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c2e2e-eaf4-43ec-a368-41f60db9ced9",
   "metadata": {},
   "source": [
    "We usually do not have precise enough information to decide a priori on a number of events that an HMP model should have. To illustrate the problem let's use the data from application 2 of this [this](https://link.springer.com/article/10.1007/s42113-021-00105-2) paper. For the purpose of this tutorial we will only use the last four participants of the data (see [this](https://www.sciencedirect.com/science/article/pii/S1053811914002249) paper for the method and [https://osf.io/pd3kf/](https://osf.io/pd3kf/) for the whole (unpreprocessed) data).\n",
    "\n",
    "In this experiment, participants performed a random-dot motion task. They were asked to indicate the direction of motion of a cloud of moving dots. While a proportion of the dots moved in a target direction, the remainder moved randomly and makes the direction discrimination more difficult. Difficulty of the task was calibrated per subject. Prior to each trial, participants received a cue that indicated whether they should respond as quickly as possible or whether they should focus on giving an accurate response: the 'speed' and 'accuracy' conditions. In this tutorial we will ignore the difference between these conditions, but in the next tutorial we will look at how we can take conditions into account in the HMP analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fa7d3",
   "metadata": {},
   "source": [
    "In this tutorial we will look at three different ways to discover and test the number of events in the data:\n",
    "- clustering\n",
    "- leave-one-out-cross validation (LOOCV)\n",
    "- bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2c6c2-63a9-4ada-a99f-218a8c2ff633",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6605bba",
   "metadata": {},
   "source": [
    "First, we load the required packages and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf03dd7-ac02-46f8-81e2-0a9fcd5bd9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import hsmm_mvpy as hmp\n",
    "from mne.io import read_info\n",
    "\n",
    "cpus=3 #set the number of cores to use for all the code\n",
    "\n",
    "# EEG data\n",
    "epoch_data = xr.load_dataset(os.path.join('../sample_data/sample_data.nc'))\n",
    "\n",
    "# channel information\n",
    "info = read_info(os.path.join('../sample_data/eeg/processed_0022_epo.fif'), verbose=False)\n",
    "\n",
    "print(epoch_data)\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc0ab5-b2be-4cf8-8a97-062fa93a31d9",
   "metadata": {},
   "source": [
    "At this point we have the epoched EEG data with 30 channels, which we need to transform to PC space. When transforming the data, a prompt requests how many PCs we want to keep. The best is to maximise the number of components kept so that we don't loose PCs with useful information. However, the more PCs we include the more computationally intensive the estimations will be. For this two rules of thumb exists, taking PCs that explain x% of the variance of the data or select the PC number at which the explained variance seems to almost stop decreasing (i.e. the \"elbow\" method).\n",
    "\n",
    "The HMP `transform_data` function will show you the two graphs below, and ask you how many PCs you would like to retain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80e9eb-1e85-431a-9e73-1056376a37fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp_data = hmp.utils.transform_data(epoch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad137c-f47b-4f7f-8d54-69bc54c31f3c",
   "metadata": {},
   "source": [
    "In this case we see that 99% explained variance is achieved with 4 PCs, and that this is coherent with when the explained variance really drops (with a larger number of electrodes than the 30 used here, 99% might be achieved with more PCs, but the elbow might occur before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a72ee-821e-4dd4-872b-7b03eb8e1266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(hmp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11090f-4276-4e5e-bb7d-051be01ae57f",
   "metadata": {},
   "source": [
    "The data is now arranged as 4 PCs x 52641 samples: all trials of all participants were concatenated for the remainder of the analysis.\n",
    "\n",
    "Finally, we need to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0879e-b78a-4083-a37b-9958b8e326ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init = hmp.models.hmp(hmp_data, epoch_data, event_width=50, cpus=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5078b-a828-48f3-9e58-5d2b3bfb808a",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196a695",
   "metadata": {},
   "source": [
    "The first method that we will use to determine the number of events that are present in the data is clustering. We introduced this method in the previous tutorial:\n",
    "- it first explores the space by sliding a single event across the trials, and stores solutions where the HMP estimation converges.\n",
    "- we repeat this with different starting points on both parameters and magnitudes to get a better appreciation of the space.\n",
    "- we can then cluster the discovered events to see how many there are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4d088",
   "metadata": {},
   "source": [
    "As the first step, we test 81 models with different parameters and magnitudes (to test only a subset, use `decimate_grid`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad04dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calc estimates\n",
    "lkhs, mags, channels, times = init.sliding_event_mags(epoch_data, step=3, decimate_grid= 1, cpu=cpus, plot=True, min_iteration=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a942811",
   "metadata": {},
   "source": [
    "The plot shows five clusters at different times, where the fourth one is weaker than the others, both in how often it was discovered and in likelihood. We store the likelihoods, times, topologies (channels), and magnitudes of all solutions.\n",
    "\n",
    "Next, we use kmeans clustering, taking likelihood, time-of-occurence, and magnitudes into account. We also remove outliers based on the mahalanobis distance. The clustering method proposes a number of clusters based on the silhouette coefficients. If you agree with the solution (5) - as we do here - enter 0, if not, enter the number of clusters you would like to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19161a-592a-4e77-a4d3-903ff8c75a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster in time, lkh, and mags\n",
    "mags_cl, pars_cl = hmp.clusters.cluster_events(init, lkhs, mags, channels, times, method='time_x_lkh_x_mags', max_clust=10, p_outlier=.05, info=info, calc_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e542eb2",
   "metadata": {},
   "source": [
    "Finally, we fit the selected model using the magnitudes and parameters of the clustering as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d54cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fit final model\n",
    "best_estimate = init.fit_single(mags_cl.shape[0], magnitudes=mags_cl,parameters=pars_cl)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, best_estimate, info, init, magnify=1, sensors=False, contours=0, event_lines=True, colorbar=True, as_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3088cab-a96f-48a1-b0d1-e6af2f960586",
   "metadata": {},
   "source": [
    "As you can see, the solution is a bit different from the results of the clustering method itself, because now it estimates five events simultaneously. To counter this problem, we developed the fit() method, also introduced in the previous tutorial. In the next section we will combine it with leave-one-out-cross validation to discover the number of events we have evidence for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad1a71",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Test what different decimations of the clustering method have for effect, by adapting the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33cfce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#estimate models\n",
    "lkhs, mags, channels, times = init.sliding_event_mags(epoch_data, step=3, decimate_grid= XYZ, cpu=cpus, plot=True, min_iteration=10)\n",
    "\n",
    "mags_cl, pars_cl = hmp.clusters.cluster_events(init, lkhs, mags, channels, times, method='time_x_lkh_x_mags', max_clust=10, p_outlier=.05, info=info, calc_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb02a3a-fe02-4bbc-b022-a5bc46454c50",
   "metadata": {},
   "source": [
    "# Leave-one-out-cross validation (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc8f23",
   "metadata": {},
   "source": [
    "The cluster method gave us an initial idea of which events occur in our data. Now we want to test statistically to what extent the different events generalize across subjects.\n",
    "\n",
    "To start, we use the `fit` method to calculate the 'maximum solution' – it will find all likely events in the data. As introduced in Tutorial 2, the ```fit``` function slides potential events from 0 to mean RT. When an event is found – the Expectation Maximization estimation converges – one event is added to the model and the slide continues. This way we can detect new events while accounting for the previous ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad2811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#estimate maximal model using fit\n",
    "max_model = init.fit()\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, max_model, info, init, as_time=True,contours=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7b578-48a5-4a1d-b0c9-e08789484d07",
   "metadata": {},
   "source": [
    "Here the `fit` method found six events – one more than with the clustering method. Next, we use `backward_estimation` – as introduced in the previous tutorial – to generate probable solutions with different numbers of events. We use the 'maximum solution' from the `fit` function as the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0455be9-6506-49c5-b3cc-297a4d2b7c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backward_model = init.backward_estimation(max_fit=max_model)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, backward_model, info, init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ba8fe",
   "metadata": {},
   "source": [
    "These results clearly show the 'strongest' events, and just by looking at the graph one might opt for either the three or four event model. To determine which of the models really describes the data best, one might be inclined to look at the raw likelihood of the solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ae4bd-710f-4e17-b026-38fd256c545c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(backward_model.n_events, backward_model.likelihoods,'o-')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.xlabel('N-event model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fe14a-9dfd-443e-b328-6e58af51f4b4",
   "metadata": {},
   "source": [
    "Unfortunately, adding more parameters will almost always improve the fit (except when events start pushing each other away because of the defined minimum duration), here up to five events.\n",
    "\n",
    "Hence we need a way to penalize the likelihood if solutions do not generalize to all participants. One way to do this is to perform leave-one out cross validation (LOOCV). This approach consists of fitting the model to all participants but one, and evaluate the likelihood of the left out participant given the estimated parameters on the other participants. The function ```hmp.utils.loocv``` does exactly this. If we give it any model as input (including the results of backward estimation), it will fit models with the given model as starting points on _n-1_ participants and estimate the likelihood of the _nth_ participant, and repeat this for all _n_.\n",
    "\n",
    "Note that LOOCV can be slow when there are many participants, in this case it might be worth it to add multiprocessing through the cpus parameter. However, this increases the memory use, so try to balance CPU use and memory depending on your situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca145a55-31a5-4894-9368-cc122f51d730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loocv_model = hmp.utils.loocv(init, hmp_data, backward_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d30d7e-25c4-4ef9-a309-f63922451cb8",
   "metadata": {},
   "source": [
    "We can then plot the result from the LOOCV procedure, with the likelihood per participant in the graph on the left, and the changes in likelihood on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c42aa-7fc0-476f-8100-ba4a10cb572d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_loocv(loocv_model, pvals=True, test='sign', indiv=True, mean=True) #Colored shaded lines represent individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a47b77-e832-4847-8216-7e03bccea887",
   "metadata": {},
   "source": [
    "Unlike in the graph above, the loglikelihood increases only up to 3 events, although with 4 participants the results are obviously not to be taken too seriously. The figure on the right contains p-values for sign tests evaluating whether the fit improved for a significant number of subjects, but this cannot be significant with 4 participants. In this case going 3 events is the best option (see right panel), considering that it has the most events for which a significant number of participants improved (4/4). In normal-sized datasets, one can use the sign-test to see how many events generalize across subjects, and that is also typically done (see also [this paper](https://link.springer.com/article/10.1007/s42113-021-00105-2) for further discussion on this topic).\n",
    "\n",
    "We can now plot the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763257f-acc5-4a8e-bd3f-98e2b69b4ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_model = backward_model.sel(n_events=3)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, final_model, info, init, as_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d77ff-d7d7-49c5-acb8-b239d0b7758f",
   "metadata": {},
   "source": [
    "This method is therefore clearly more conservative than the clustering method (and than the maximal model from `fit`), at least with so few participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88254a7",
   "metadata": {},
   "source": [
    "You might have noticed that the `loocv` method printed a warning saying that it is incorrect. The reason for this is that the input model is used to inform the models fitted to _n-1_ participants, while this input model was based on all participants. Therefore, the calculated loglikelihood of the _nth_ subject is not completely independent of the other participants. We do provide this function, because it a) provides a quick way to estimating rough LOOCV values and b) can be used if you have independent starting points, perhaps from the literature or other studies in your lab.\n",
    "\n",
    "Of course, we also included a correct method. `loocv_func` will apply a given function (for example, backward estimation) to _n-1_ participants, and then calculate the fit. Unforunately, as in this case we don't have useful starting points it will take much longer to calculate. As we typically do this for backward estimation, we also provide a function specifically for this: `loocv_backward`. In the next tutorial we will illustrate how it can be used in a more complex situation (backward estimation followed by condition-specific models). Both `loocv_func` and `loocv_backward` return a list: the first item contains the loglikelihoods, the second item the fitted models to _n-1_ participants that can be used to construct the final model.\n",
    "\n",
    "Executing the code below will take quite some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct_loocv_model = hmp.utils.loocv_backward(init, hmp_data, max_events=6)\n",
    "hmp.visu.plot_loocv(correct_loocv_model[0], pvals=True, test='sign', indiv=True, mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7ee95",
   "metadata": {},
   "source": [
    "Not only are these results correct, but they are also smoother than the ones before. Now, we might go for a four event model, as 3 out of 4 participants still improved. However, we certainly have evidence for 3 events here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d5a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = correct_loocv_model[1]\n",
    "\n",
    "#get average mags and params\n",
    "mags = models[0].sel(n_events=3).dropna('event').magnitudes\n",
    "params = models[0].sel(n_events=3).dropna('stage').parameters\n",
    "for pp in range(1,len(np.unique(models[0].participant.values))):\n",
    "    mags += models[pp].sel(n_events=3).dropna('event').magnitudes\n",
    "    params += models[pp].sel(n_events=3).dropna('stage').parameters\n",
    "mags = mags / len(np.unique(models[0].participant.values))\n",
    "params = params / len(np.unique(models[0].participant.values))\n",
    "\n",
    "#fit final model\n",
    "correct_final_model = init.fit_single(3,magnitudes=mags,parameters=params)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, correct_final_model, info, init, as_time=True, contours=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cc1d2",
   "metadata": {},
   "source": [
    "which is, as expected, very similar to the one above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec703a",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "The `loocv` method can deal with any model as input. Fit a 3-event and a 4-event model on all data, and use them as input to `loocv`. Then, compare the likelihoods of the for participants to see which participants improve when adding one event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db972f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fit models with three and four events\n",
    "model3 = ...\n",
    "model4 = ...\n",
    "\n",
    "#perform loocv\n",
    "loocv = hmp.utils.loocv(...,  [model3, model4])\n",
    "\n",
    "#explore likelihoods\n",
    "loocv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236f2e",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "If you feel adventurous, you can repeat Exercise 2, but now do it correctly with `loocv_func`. Check it's docstring to see how it can be used, and use `example_fit_single_func` as your input function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925efe9-74bd-42ab-bf20-f9563e0095cf",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c787865-b6eb-4b03-8067-586d433226e3",
   "metadata": {},
   "source": [
    "At this point, we are certain that there is evidence in the data for at least three events. As a final step of judging the quality of this solution and estimating variability in the data due to noise and different strategies, for example, we perform bootstrapping. The idea of [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) is to test the model on random samples (with replacement) of our data, to make sure that the discovered events are robust.\n",
    "\n",
    "In the case of EEG data we can bootstrap the epochs/trials of the EEG data. Thus, the analysis consists of three steps that are repeated a number of times:\n",
    "1. generate a bootstrapped dataset\n",
    "2. fit a model using `fit_single` in combination with the parameters and magnitudes of the winning LOOCV model\n",
    "3. record the parameters of the fitted HMP model\n",
    "\n",
    "With sufficient bootstrapped repetitions we will know wich events tend to be rare (e.g., outliers or different strategies) and what the variance in time and topologies are.\n",
    "\n",
    "To illustrate, we first run a single repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620f021-26fc-4b1f-ab35-17039429f468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use final model as input\n",
    "bootstrapped = hmp.resample.bootstrapping(fit=final_model, data=epoch_data, \n",
    "                                          dim=['epochs','participant'], n_iterations=1,\n",
    "                                          init=init, cpus=cpus, pca_weights=hmp_data.pca_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd8f76-de77-4849-9531-00aaa2225781",
   "metadata": {},
   "source": [
    "We then compare to the times of the final LOOCV model (the vertical lines in the plot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c370a1a-f88b-4498-abe9-23bb65aec067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "times_general_run = init.compute_times(init, correct_final_model, duration=False, mean=True, add_rt=True)\n",
    "hmp.visu.plot_topo_timecourse(bootstrapped.sel(iteration=0).channels_activity.values, \n",
    "                              bootstrapped.sel(iteration=0).event_times.values,\n",
    "                              info, init, times_to_display=times_general_run.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08845d",
   "metadata": {},
   "source": [
    "As you can see mainly the last event changes. But one bootstrap is not very informative, \n",
    "if we run 10 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9486690-0a2a-4242-8939-0598e361dfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrapped = hmp.resample.bootstrapping(fit=correct_final_model, data=epoch_data, \n",
    "                                          dim=['epochs','participant'], n_iterations=10,\n",
    "                                          init=init, cpus=cpus, pca_weights=hmp_data.pca_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc6d8e-fec1-4072-8326-8c9642c251b0",
   "metadata": {},
   "source": [
    "We can then plot the bootstrapped models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d99c3-7c14-4c20-92bf-11a938da5c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(bootstrapped.channels_activity.values, \n",
    "                              bootstrapped.event_times.values,\n",
    "                              info, init, times_to_display=times_general_run.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4586d-8825-42ac-bb62-bc64d62803fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:31.511282Z",
     "iopub.status.busy": "2023-10-16T10:49:31.511152Z",
     "iopub.status.idle": "2023-10-16T10:49:38.917630Z",
     "shell.execute_reply": "2023-10-16T10:49:38.917211Z",
     "shell.execute_reply.started": "2023-10-16T10:49:31.511269Z"
    },
    "tags": []
   },
   "source": [
    "### Exercise 4:\n",
    "\n",
    "#### Exercise 4.1 on variability\n",
    "The plot above shows mainly a change in the last event. What inference can you draw on this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45e840-b061-458f-a3fb-8329510cab4f",
   "metadata": {},
   "source": [
    "The bootstrapped object contains all the bootstrapped run and we can use it to inspect different properties of the bootstrapped sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438090eb-e798-4908-ab60-07248f2ec1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bootstrapped.sel(event=[2]).event_times.std())\n",
    "print(bootstrapped.sel(stage=[3]).parameters.prod(axis=1).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64be1cd-bf77-4899-aec5-a7253de91adc",
   "metadata": {},
   "source": [
    "#### Exercise 4.2 how to spice your bootstraps\n",
    "\n",
    "What dimension(s) can you think would be interesting to bootstrap on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6288af-8e60-4de4-8c1c-dff396fe2184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrapped = hmp.resample.bootstrapping(fit=correct_final_model, data=epoch_data, \n",
    "                                          dim=[...], n_iterations=10,\n",
    "                                          init=init, cpus=cpus, pca_weights=hmp_data.pca_weights)\n",
    "hmp.visu.plot_topo_timecourse(bootstrapped.channels_activity.values, \n",
    "                              bootstrapped.event_times.values,\n",
    "                              info, init, times_to_display=times_general_run.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c9a06",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "- we explored the likelihood landscape using the clustering method\n",
    "- used LOOCV to determine statistically for how many events there is evidence in the data\n",
    "- used bootstrapping to investigate the variability of the winning LOOCV model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
