{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da822472-1456-44cc-9ddc-d7138ecd50a1",
   "metadata": {},
   "source": [
    "# Estimating a model with a known number of events\n",
    "\n",
    "This section will show you how to estimate a single HMP model with a given number of event as in the previous but we'll make it harder to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ddd19-5e27-407a-89d3-b48081434198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Importing the package\n",
    "import hsmm_mvpy as hmp\n",
    "#Plotting \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cpus = 3 # For multiprocessing, usually a good idea to use multiple CPus as long as you have enough RAM\n",
    "\n",
    "## Running the demo function in the simulation module\n",
    "# Inspect the given function to apply your own simulations\n",
    "from hsmm_mvpy.simulations import demo\n",
    "number_of_events = 8 #how many events to simulate\n",
    "epoch_data, random_source_times, info = demo(cpus, number_of_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9b80a-cb18-4951-8c0b-886bea342f53",
   "metadata": {},
   "source": [
    "Then we transform the data:\n",
    "- Standardize individual variances (not in this case as only one participant is simulated)\n",
    "- Reducing dimensionality by selecting a number of principal component from the spatial PCA applied to the variance-covariance matrix of the channel activity\n",
    "- zscoring the data of each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaccc5e-ac16-408b-a63f-60b2b087ae58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp_dat = hmp.utils.transform_data(epoch_data, apply_standard=False, n_comp=4)\n",
    "print(hmp_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c464621-b9b0-467b-94cc-e9203b22a391",
   "metadata": {},
   "source": [
    "HMP uses classes to instantiate fit of hmp models. Therefore before estimating a model one needs to declare an instance of the hmp class by specifying the data, the sampling frequency of the data as well as several parmeters such as the event width (in milliseconds) and how many CPU to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c1e08-22ab-437b-aaa6-7b608514a8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init = hmp.models.hmp(hmp_dat, sfreq=epoch_data.sfreq, event_width=50, cpus=cpus, location=25)#Initialization of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bb4d1-9990-4376-aad6-edbdef876892",
   "metadata": {},
   "source": [
    "Once the class has been initiated, the function ```fit_single()``` can be used to estimate one hmp model with a specified number of transition events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef91e86-6d4e-4ccb-a9df-313aa69be800",
   "metadata": {},
   "source": [
    "## Direct estimation\n",
    "We can simply call the ```fit_single``` method from the hmp class to estimate a given number of events. The hmp algorithm will then look for 4 events. Now the expectation maximization algorithm to fit the model is in this case sensitive to the starting points given to the stage duration. By default, the ```fit_single``` method uses a starting point where all events are equally distributed between 0 and the mean RT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16750502-c8fe-4943-93c4-70ed77410868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fitting\n",
    "selected = init.fit_single(number_of_events)#function to fit an instance of a 10 events model\n",
    "\n",
    "#Visualizing\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aff90f-b299-4edb-a00b-0ac73d5171d1",
   "metadata": {},
   "source": [
    "And we can take a look at the traces from the expectation maximization procedure showing us how many iterations it needed and at what Log-likelihood (LL) it stopped according to the default tolerance on the increase in LL (default 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4be215-ed77-4d35-b493-c5c04c0d09dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(selected.traces)\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.xlabel('EM iteration');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d29ae0-eb56-46fe-aede-44759bb45f49",
   "metadata": {},
   "source": [
    "Now launching a single model without additional starting points is not a good idea as often the default starting values (illustrated below) of the expectation maximization algorithm will end up in a local minima: \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f85c43-a3df-4fec-95e8-6ade007af71f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fitting\n",
    "selected = init.fit_single(number_of_events, maximization=False)#function to fit an instance of a 10 events model\n",
    "\n",
    "#Visualizing\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfe178-b9e5-4e3e-b3b0-57cb0ab6f271",
   "metadata": {},
   "source": [
    "We need to use more starting points!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaac51b-dcb0-41b2-98cb-229598ffe405",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Adapt the cell below to generate starting points on the scale of the gamma parameters to capture all those events.\n",
    "\n",
    "Keep in mind that the mean of a gamma is _scale * shape_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f73f7-d488-4e43-a7f4-ddeaaf701245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pars = np.reshape(np.concatenate([\n",
    "    np.repeat(init.shape, 9), \n",
    "    [1,1,1,1,1,1,1,1,1]]),#Replace values here\n",
    "                       [2,9]).T\n",
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cabf53-2bbc-47d6-9c56-57bf57c91659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fitting\n",
    "selected = init.fit_single(number_of_events, parameters=pars)#function to fit an instance of a 10 events model\n",
    "\n",
    "#Visualizing\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16f166-4d94-4895-a5bd-400a7f37d5a0",
   "metadata": {},
   "source": [
    "## Random method\n",
    "\n",
    "\n",
    "A better idea is to run a single model with several starting points and selecting the result with the best fit. This can be declared in the ```single_fit()``` function; hereby an example with 100 random starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6feb0e-7e2d-4905-aad6-e65255cea7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NOTE: if you run this in VS code on a Mac in an interactive window, you might get strange behavior (running of previous processes) due to the multiprocessing. If so, set your multiprocessing start method to 'fork' (which is the default on Unix) by uncommenting the next two lines:\n",
    "#import multiprocessing as mp\n",
    "#mp.set_start_method(\"fork\")\n",
    "\n",
    "# Fitting\n",
    "selected = init.fit_single(number_of_events, method='random', starting_points=100)#function to fit an instance of a 4 events model\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed28af16-d0b3-48f7-b7b0-f7d0949278b1",
   "metadata": {},
   "source": [
    "But, by definition, the starting points are random so inducing a lot of redundancy and also taking the risk that some points in the parameter space remain unexplored. Several calls to this function will not always give the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2cd01-e6a1-4509-8dce-dd154b0815ce",
   "metadata": {},
   "source": [
    "### Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff912579-b958-4cdb-a685-92e700f3a031",
   "metadata": {},
   "source": [
    "What good solution for this can you think of? To illustrate where we are going, using default starting points fit models by subsampling from 1 to 12 events (e.g. 2, 4, 10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112f486-6f44-4e99-829a-9d5bd4c90b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = init.fit_single(number_of_events)#Replace number_of_events with the desired number\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ed170-ee42-46b1-800f-8462bd67548a",
   "metadata": {},
   "source": [
    "## Backward estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840f7ad-5459-43da-b083-dd9b9101a8de",
   "metadata": {},
   "source": [
    "Another solution than using random points is to first estimate a model with the maximal number of possible events (referred to as 'the maximal model'), and then decrease the number of events one by one.\n",
    "\n",
    "The idea is that genuine events will necessarily be found at the expected locations in the maximal model. Because the backward estimation method iteratively removes the weakest event (in terms of likelihood), only the 'strongest' events remain even if there location would have been hard to find with a single fit and default starting values.\n",
    "\n",
    "In order to do this we will use the ```backward_estimation()``` function. This function first estimates the maximal model (defined based on the event width and the minimum reaction time), then estimates the max_event - 1 solution by iteratively removing one of the events and picking the next solution with the highest likelihood (for more details see Borst & Anderson, [2021](http://jelmerborst.nl/pubs/ACTR_hmp_MVPA_BorstAnderson_preprint.pdf)) and repeat this until the 1 event solution is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ac9c8-a973-4c62-8313-a80e22e2dff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "bests = init.backward_estimation(max_events=int(init.compute_max_events()/2)) #/2 to go faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee3876-a91a-430e-831c-2c7ac7cba16f",
   "metadata": {},
   "source": [
    "Here we plot the resulting solutions going from the maximal possible number of events that fit into the minimum RT given a minimum duration of 25ms in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcca53-f150-42dc-9c21-91e111fa8169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, bests, info, init, sensors=False,\n",
    "                    times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45fc8a-ea6e-4f34-91c5-4361580fd4e0",
   "metadata": {},
   "source": [
    "And from these solutions we can then select the number of events we originally wanted to estimate (which is the correct solution in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0dffa-a2b2-4b20-8f91-9416e65616c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = bests.sel(n_events=number_of_events)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05741c-7ded-46ff-8293-ab7e2de9817d",
   "metadata": {},
   "source": [
    "The downside is that we are still unsure about whether we included all possible starting points in the mix. In addition, this method can be suboptimal with 1) long RTs and therefore a lot of events to fit and long computation times and 2) if there is a big difference between the minimum RT (determining the maximum number of events) and the mean RT (all possible locations of these events)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658864c9-b5b5-4dad-94b7-b2c181108680",
   "metadata": {},
   "source": [
    "# Sliding events\n",
    "\n",
    "To account for the second point in the previous comment, one possibility is to estimate a single event model and test the landing point in the log-likelihood. The sliding event works by sliding a single event separating the mean reaction time into two stages with the first one ranging from 0 to the mean reaction time. Discontinuities in the resulting log-likelihood usually shows that there is an event around the corresponding times. Note though that it is not a perfect process and a very high variance event can cover a near lower variance event (e.g. between sample number 180 and 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195935d-6090-4e25-8680-e00067555f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init.sliding_event(fix_pars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2aea92-af68-4cb1-8d36-2690acbb7f7c",
   "metadata": {},
   "source": [
    "Now the previous curve is obtaine by fixing the parameters of the two gammas and only estimating channel contribution but if we relax this constrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828374da-9f82-4237-a5cb-e3c6473bd96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init.sliding_event(fix_pars=False, step=3)#step 3 test every 3rd sample "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df3eda-4805-494f-bd83-7459ba153c11",
   "metadata": {},
   "source": [
    "Then we see that each sample number used as starting point will go to its nearest local minima. But based on the number of landing points we see, we did not capture all the events that we simulated (8). The reason is that, while we did explore the parameter space of the gammas, we didn't test for different starting points in the channel contributions. If we generate a lot of magnitudes and pass it to the ```sliding_event``` function the landing points look closer to what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b1a19-dfa2-40f4-9332-591fe4c36018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "magnitudes_starting_points = init.gen_mags(n_events=1, decimate=3)#decimate reduces the possible number of starting points\n",
    "magnitudes_starting_points[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4229e8f-8c28-4659-9d83-4d5bedfb99a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init.sliding_event(magnitudes=magnitudes_starting_points, step=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1214c6-b0f1-4714-a5e4-5c1ee479c31d",
   "metadata": {},
   "source": [
    "It appears that several landings points appear with some redundancies. To group those points together we use a clustering algorithm and make a choice on the number of clusters based on the grouping quality (Silhouette and visual inspection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8cc64-165c-474b-ad41-d11126e2cbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calc estimates\n",
    "lkhs, mags, channels, times = init.sliding_event_mags(epoch_data, step=3, decimate_grid = 3, cpu=cpus, plot=True, min_iteration=10)\n",
    "\n",
    "# cluster in time, lkh, and mags\n",
    "mags_cl, pars_cl = hmp.clusters.cluster_events(init, lkhs, mags, channels, times, method='time_x_lkh_x_mags', max_clust=10, p_outlier=.05, info=info, calc_outliers=True)\n",
    "\n",
    "#fit final model\n",
    "best_estimate = init.fit_single(mags_cl.shape[0], magnitudes=mags_cl,parameters=pars_cl)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, best_estimate, info, init, magnify=1, sensors=False, time_step=1000/init.sfreq,xlabel='Time (ms)', contours=0, event_lines=True, colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072212ed-49ce-4442-afa4-76eb00d983f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, best_estimate, info, init, magnify=1, sensors=False, time_step=1000/init.sfreq, xlabel='Time (ms)', contours=0, event_lines=True, colorbar=True,\n",
    "                              times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60cfff-2346-43ac-88e7-5ce55d7863b9",
   "metadata": {},
   "source": [
    "Now the solution isn't yet perfect and can take quite some time depending on the number of Principal components (and therefore combinations of starting points) and the number of tested clusters. One possible explanation is that not taking into account the previous events makes it harder to detect the next one (probably why we are missing the last one here). Therefore we created a method combining all the properties of this way of exploring the starting points:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77643eee-ec31-4da7-bd53-cc3c33d51a36",
   "metadata": {},
   "source": [
    "## Cumulative event fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976431f-c769-4cf9-9d71-2726f8312284",
   "metadata": {},
   "source": [
    "Instead of fitting an _n_ event model, as the ```sliding_event()``` function, this method starts by fitting a 1 event model (two stages) using each sample from the time 0 (stimulus onset) to the mean RT. Therefore it tests for the landing point of the expectation maximization algorithm given each sample as starting point and the likelihood associated with this landing point. As soon as a starting points reaches the convergence criterion, the function fits an _n+1_ event model and uses the next samples in the RT for the following event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fafcbc-0b53-4e71-9082-5c43c11cb818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimates = init.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ec398-371f-4b6a-98b8-d318add0aa08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates, info, init, \n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66126dc9-4489-4c37-9db5-f3aa2662c4ab",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "What can cause the failure to capture the three successive events knowing that their topologies are quite similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5023495-1396-459b-aa42-188cbc301bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = bests.sel(n_events=number_of_events)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bdf1f-27bb-423d-a497-515ae6896a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp_dat = hmp.utils.transform_data(epoch_data, apply_standard=False)\n",
    "#Plot new PC selection\n",
    "hmp.visu.plot_components_sensor(hmp_dat, positions=info)\n",
    "#Re-Initialization of the model\n",
    "init = hmp.models.hmp(hmp_dat, sfreq=epoch_data.sfreq, event_width=50, cpus=cpus, location=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff5ec5-291a-4c13-9b82-4249722ad584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T08:59:48.187183Z",
     "iopub.status.busy": "2023-07-18T08:59:48.186676Z",
     "iopub.status.idle": "2023-07-18T08:59:48.322229Z",
     "shell.execute_reply": "2023-07-18T08:59:48.320967Z",
     "shell.execute_reply.started": "2023-07-18T08:59:48.187146Z"
    },
    "tags": []
   },
   "source": [
    "Reperform the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634349e-9652-4444-b89a-795cd37f3516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimates = init.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c59f7-7562-4f77-8944-ee04f314eec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates, info, init, \n",
    "                                times_to_display = np.mean(np.cumsum(random_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dcb180-cda4-4cd5-b30e-8aaf66580bc7",
   "metadata": {},
   "source": [
    "# Data saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd030f8-461a-433d-9f31-ee440e4be43b",
   "metadata": {},
   "source": [
    "Once finished we can save fitted models using the dedicated command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0c3f1-062b-40f1-a49f-af6f3845895a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.utils.save_fit(selected, 'selected.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87214cd-3795-4827-b6c9-249d5e6cf27f",
   "metadata": {},
   "source": [
    "And load the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d5e99-cd2f-4553-9ca3-1c56e01a04c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimates = hmp.utils.load_fit('selected.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afd3e4-8467-4e25-963a-5de8f44447a6",
   "metadata": {},
   "source": [
    "Or even only save the estimated event probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c3b24-d271-4bd3-a7a8-fe552c087b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.utils.save_eventprobs(selected.eventprobs, 'selected_eventprobs.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
