{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da822472-1456-44cc-9ddc-d7138ecd50a1",
   "metadata": {},
   "source": [
    "# Estimating a model with a known number of events\n",
    "\n",
    "This section will show you how to estimate a single HMP model with a given number of events, as in the previous tutorial, but now we'll make it more realistic and harder to discover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018ba70-9b3d-48e5-b845-219de7408080",
   "metadata": {},
   "source": [
    "First we do more or less the same as in the previous tutorial:\n",
    "- simulate EEG data\n",
    "- transform the data\n",
    "- initialize a model with default parameters for distribution, shape and event duration\n",
    "\n",
    "We now simulate the data by calling the `demo` function, which is only used for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ddd19-5e27-407a-89d3-b48081434198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:27.015091Z",
     "iopub.status.busy": "2023-10-16T10:49:27.014955Z",
     "iopub.status.idle": "2023-10-16T10:49:29.042034Z",
     "shell.execute_reply": "2023-10-16T10:49:29.041483Z",
     "shell.execute_reply.started": "2023-10-16T10:49:27.015076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Importing the package\n",
    "import hsmm_mvpy as hmp\n",
    "#Plotting \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cpus = 3 # For multiprocessing, usually a good idea to use multiple CPUs as long as you have enough RAM\n",
    "\n",
    "## Running the demo function in the simulation module\n",
    "# Inspect the given function to apply your own simulations\n",
    "from hsmm_mvpy.simulations import demo\n",
    "n_events = 8 #how many events to simulate\n",
    "epoch_data, sim_source_times, info = demo(cpus, n_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375af5ff",
   "metadata": {},
   "source": [
    "We specify directly that we want 4 PC components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaccc5e-ac16-408b-a63f-60b2b087ae58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:29.043804Z",
     "iopub.status.busy": "2023-10-16T10:49:29.043387Z",
     "iopub.status.idle": "2023-10-16T10:49:29.141423Z",
     "shell.execute_reply": "2023-10-16T10:49:29.140997Z",
     "shell.execute_reply.started": "2023-10-16T10:49:29.043786Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp_dat = hmp.utils.transform_data(epoch_data, apply_standard=False, n_comp=4)\n",
    "print(hmp_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cb5c2",
   "metadata": {},
   "source": [
    "And we initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c1e08-22ab-437b-aaa6-7b608514a8f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:29.142390Z",
     "iopub.status.busy": "2023-10-16T10:49:29.142129Z",
     "iopub.status.idle": "2023-10-16T10:49:29.163667Z",
     "shell.execute_reply": "2023-10-16T10:49:29.162761Z",
     "shell.execute_reply.started": "2023-10-16T10:49:29.142373Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init = hmp.models.hmp(hmp_dat, sfreq=epoch_data.sfreq, event_width=50, cpus=cpus)#Initialization of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bb4d1-9990-4376-aad6-edbdef876892",
   "metadata": {},
   "source": [
    "Once the class has been initiated, the function ```fit_single()``` can be used to estimate one hmp model with a specified number of transition events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef91e86-6d4e-4ccb-a9df-313aa69be800",
   "metadata": {},
   "source": [
    "## Direct estimation\n",
    "As in the previous tutorial, we can simply call the ```fit_single``` method from the hmp class to estimate a given number of events. The hmp algorithm will then look for ```n_event``` events. \n",
    "\n",
    "Unfortunately, the expectation maximization algorithm to fit the model is sensitive to the starting points given to the stage duration. By default, the ```fit_single``` method uses a starting point where all events are equally distributed between 0 and the mean RT, and the magnitudes are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16750502-c8fe-4943-93c4-70ed77410868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:29.165454Z",
     "iopub.status.busy": "2023-10-16T10:49:29.164973Z",
     "iopub.status.idle": "2023-10-16T10:49:29.954862Z",
     "shell.execute_reply": "2023-10-16T10:49:29.954486Z",
     "shell.execute_reply.started": "2023-10-16T10:49:29.165416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fitting\n",
    "selected = init.fit_single(n_events)#function to fit an instance of a 10 events model\n",
    "\n",
    "#Visualizing\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d29ae0-eb56-46fe-aede-44759bb45f49",
   "metadata": {},
   "source": [
    "As you can see, the method discovers 8 events - as we told it to - but one of them is at the wrong location (the vertical lines indicate the true event locations).\n",
    "\n",
    "Thus, launching a single model without additional starting points is not a good idea as sometimes the default starting values of the expectation maximization algorithm will end up in a local minima, as above.\n",
    "\n",
    "To see what happens if we just use the default inputs and not maximize at all, we can set maximization to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f85c43-a3df-4fec-95e8-6ade007af71f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:29.955720Z",
     "iopub.status.busy": "2023-10-16T10:49:29.955518Z",
     "iopub.status.idle": "2023-10-16T10:49:30.612318Z",
     "shell.execute_reply": "2023-10-16T10:49:30.612001Z",
     "shell.execute_reply.started": "2023-10-16T10:49:29.955698Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fitting\n",
    "selected = init.fit_single(n_events, maximization=False)#function to fit an instance of a 10 events model\n",
    "\n",
    "#Visualizing\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False, times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfe178-b9e5-4e3e-b3b0-57cb0ab6f271",
   "metadata": {},
   "source": [
    "As expected, the 'discovered' events are divided equally over the mean RT. The magnitudes are not 0, as we're plotting the average EEG data at the discovered time points in each trial - as you can see these averages are quite random, and also have very small amplitudes compared to the graph above.\n",
    "\n",
    "The lesson we learned: we need to use more starting points!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaac51b-dcb0-41b2-98cb-229598ffe405",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Adapt the cell below to generate starting points on the scale of the gamma parameters to capture all those events. Compare the discovered events to the vertical lines - the onset of the events should overlap with the lines.\n",
    "\n",
    "Keep in mind that the mean of a gamma is _scale * shape_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f73f7-d488-4e43-a7f4-ddeaaf701245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:30.612858Z",
     "iopub.status.busy": "2023-10-16T10:49:30.612729Z",
     "iopub.status.idle": "2023-10-16T10:49:30.616673Z",
     "shell.execute_reply": "2023-10-16T10:49:30.616426Z",
     "shell.execute_reply.started": "2023-10-16T10:49:30.612848Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pars = np.reshape(np.concatenate([\n",
    "    np.repeat(init.shape, 9), \n",
    "    #Following values are our starting points for the stage durations\n",
    "    [10,10,10,10,10,10,10,10,10]]),#Replace values here- for the exercise\n",
    "    [2,9]).T\n",
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cabf53-2bbc-47d6-9c56-57bf57c91659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:30.617919Z",
     "iopub.status.busy": "2023-10-16T10:49:30.617736Z",
     "iopub.status.idle": "2023-10-16T10:49:31.510683Z",
     "shell.execute_reply": "2023-10-16T10:49:31.510357Z",
     "shell.execute_reply.started": "2023-10-16T10:49:30.617908Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fitting\n",
    "selected = init.fit_single(n_events, parameters=pars)#function to fit an instance of a 10 events model\n",
    "\n",
    "#Visualizing\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16f166-4d94-4895-a5bd-400a7f37d5a0",
   "metadata": {},
   "source": [
    "## Random method\n",
    "\n",
    "In reality, we do not know where the real events are. Thus, a better idea is to run a single model with several starting points and selecting the result with the best fit. This can be declared in the ```single_fit()``` function; here we estimate an example with 100 random starting points. We set `return_max` to False to get all models back, instead of only the winning one. \n",
    "\n",
    "We then sort the models by log-likelihood - how well they explain the EEG data - and plot every 9th model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd558db-f9aa-4bc2-8b5e-b3d5f0e03b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T13:22:33.669429Z",
     "iopub.status.busy": "2023-10-16T13:22:33.669253Z",
     "iopub.status.idle": "2023-10-16T13:22:48.838290Z",
     "shell.execute_reply": "2023-10-16T13:22:48.837870Z",
     "shell.execute_reply.started": "2023-10-16T13:22:33.669418Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting\n",
    "selected = init.fit_single(n_events, method='random', starting_points=100,\n",
    "                           return_max=False)#function to fit an instance of a 4 events model\n",
    "\n",
    "plt.plot(selected.sortby('likelihoods').likelihoods)#Sorted SP iteration based on log-likelihood\n",
    "plt.xlabel('(sorted) Iteration')\n",
    "plt.ylabel('Log-likelihood')\n",
    "for iteration in selected.sortby('likelihoods').iteration[::9]:#Only plot every 9 model\n",
    "    hmp.visu.plot_topo_timecourse(epoch_data, selected.sel(iteration=iteration), info, init, magnify=1, sensors=False, times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e720b1e-7c69-4f9f-bf13-ff851d8fb889",
   "metadata": {},
   "source": [
    "As you can see, the higher the likelihood, the closer the discovered events are to the real events, and the most likely solution is indeed the correct one. Instead of recording all those starting points we can just take the most likely one directly by setting `return_max` to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6feb0e-7e2d-4905-aad6-e65255cea7c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:31.511282Z",
     "iopub.status.busy": "2023-10-16T10:49:31.511152Z",
     "iopub.status.idle": "2023-10-16T10:49:38.917630Z",
     "shell.execute_reply": "2023-10-16T10:49:38.917211Z",
     "shell.execute_reply.started": "2023-10-16T10:49:31.511269Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NOTE: if you run this in VS code on a Mac in an interactive window, you might get strange behavior (running of previous processes) due to the multiprocessing. If so, set your multiprocessing start method to 'fork' (which is the default on Unix) by uncommenting the next two lines:\n",
    "#import multiprocessing as mp\n",
    "#mp.set_start_method(\"fork\")\n",
    "\n",
    "# Fitting\n",
    "selected = init.fit_single(n_events, method='random', starting_points=100, \n",
    "                           return_max=True)#function to fit an instance of a 4 events model\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed28af16-d0b3-48f7-b7b0-f7d0949278b1",
   "metadata": {},
   "source": [
    "But, by definition, the starting points are random: as a result they induce a lot of redundancy and you take the risk that some points in the parameter space remain unexplored. Several calls to this function will not always give the correct solution if not enough starting points have been provided. Below we will explore better/additional solutions to this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2cd01-e6a1-4509-8dce-dd154b0815ce",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "What good solution for this can you think of? To illustrate where we are going, fit models by subsampling from 1 to 12 events (e.g. 2, 4, 10, 12) with default starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112f486-6f44-4e99-829a-9d5bd4c90b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:38.918414Z",
     "iopub.status.busy": "2023-10-16T10:49:38.918288Z",
     "iopub.status.idle": "2023-10-16T10:49:39.193856Z",
     "shell.execute_reply": "2023-10-16T10:49:39.193523Z",
     "shell.execute_reply.started": "2023-10-16T10:49:38.918402Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = init.fit_single(2)#Replace 2 with the desired number\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, magnify=1, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ed170-ee42-46b1-800f-8462bd67548a",
   "metadata": {},
   "source": [
    "## Backward estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840f7ad-5459-43da-b083-dd9b9101a8de",
   "metadata": {},
   "source": [
    "Another solution than using random points is to first estimate a model with the maximal number of possible events that fit in RTs (referred to as 'the maximal model'), and then decrease the number of events one by one.\n",
    "\n",
    "The idea is that genuine events will necessarily be found at the expected locations in the maximal model. Because the backward estimation method iteratively removes the weakest event (in terms of likelihood), only the 'strongest' events remain even if there location would have been hard to find with a single fit and default starting values.\n",
    "\n",
    "In order to do this we will use the ```backward_estimation()``` function. This function first estimates the maximal model (defined based on the event width and the minimum reaction time), then estimates the max_event - 1 solution by iteratively removing one of the events and picking the next solution with the highest likelihood (for more details see Borst & Anderson, [2021](http://jelmerborst.nl/pubs/ACTR_hmp_MVPA_BorstAnderson_preprint.pdf)) and repeat this until the 1 event solution is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ac9c8-a973-4c62-8313-a80e22e2dff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:39.194510Z",
     "iopub.status.busy": "2023-10-16T10:49:39.194403Z",
     "iopub.status.idle": "2023-10-16T10:49:47.944066Z",
     "shell.execute_reply": "2023-10-16T10:49:47.943514Z",
     "shell.execute_reply.started": "2023-10-16T10:49:39.194501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "bests = init.backward_estimation(max_events=int(init.compute_max_events()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee3876-a91a-430e-831c-2c7ac7cba16f",
   "metadata": {},
   "source": [
    "Here we plot the resulting solutions going from the maximal possible number of events (16) all the way to a single event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcca53-f150-42dc-9c21-91e111fa8169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:47.944764Z",
     "iopub.status.busy": "2023-10-16T10:49:47.944654Z",
     "iopub.status.idle": "2023-10-16T10:49:57.745568Z",
     "shell.execute_reply": "2023-10-16T10:49:57.745185Z",
     "shell.execute_reply.started": "2023-10-16T10:49:47.944754Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, bests, info, init, sensors=False,\n",
    "                    times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45fc8a-ea6e-4f34-91c5-4361580fd4e0",
   "metadata": {},
   "source": [
    "As you can see, the 16-event solution finds the real events + some extra ones (which typically look very weak). From these solutions we can select the number of events we originally wanted to estimate (which is the correct solution in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0dffa-a2b2-4b20-8f91-9416e65616c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:57.746007Z",
     "iopub.status.busy": "2023-10-16T10:49:57.745908Z",
     "iopub.status.idle": "2023-10-16T10:49:58.327353Z",
     "shell.execute_reply": "2023-10-16T10:49:58.326721Z",
     "shell.execute_reply.started": "2023-10-16T10:49:57.745997Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = bests.sel(n_events=n_events)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05741c-7ded-46ff-8293-ab7e2de9817d",
   "metadata": {},
   "source": [
    "The downside is that we are still unsure about whether we included all possible starting points in the mix. In addition, this method can be suboptimal with 1) long RTs and therefore a lot of events to fit and long computation times and 2) if there is a big difference between the minimum RT (determining the maximum number of events) and the mean RT (all possible locations of these events)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658864c9-b5b5-4dad-94b7-b2c181108680",
   "metadata": {},
   "source": [
    "# Sliding events\n",
    "\n",
    "To account for the second point in the previous comment, one possibility is to estimate a single event model and test the landing point in the log-likelihood. The sliding event method works by sliding a single event separating the mean reaction time into two stages. Discontinuities in the resulting log-likelihood usually shows that there is an event around the corresponding times. Note though that it is not a perfect process and a very high amplitude event can cover a near lower amplitude event (e.g. between sample number 18 and 25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195935d-6090-4e25-8680-e00067555f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:58.327936Z",
     "iopub.status.busy": "2023-10-16T10:49:58.327825Z",
     "iopub.status.idle": "2023-10-16T10:49:58.804577Z",
     "shell.execute_reply": "2023-10-16T10:49:58.804039Z",
     "shell.execute_reply.started": "2023-10-16T10:49:58.327926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init.sliding_event(fix_pars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2aea92-af68-4cb1-8d36-2690acbb7f7c",
   "metadata": {},
   "source": [
    "The previous curve is obtained by fixing the parameters of the two gammas and only estimating channel contributions. We can also relax this constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828374da-9f82-4237-a5cb-e3c6473bd96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:58.805938Z",
     "iopub.status.busy": "2023-10-16T10:49:58.805575Z",
     "iopub.status.idle": "2023-10-16T10:49:59.150515Z",
     "shell.execute_reply": "2023-10-16T10:49:59.149990Z",
     "shell.execute_reply.started": "2023-10-16T10:49:58.805913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init.sliding_event(fix_pars=False, step=3)#step 3 test every 3rd sample "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df3eda-4805-494f-bd83-7459ba153c11",
   "metadata": {},
   "source": [
    "Then we see that each sample number used as starting point will go to its nearest local minima. But based on the number of landing points we see, we did not capture all the events that we simulated (8). The reason is that, while we did explore the parameter space of the gammas, we didn't test for different starting points in the channel contributions. If we generate a lot of magnitudes and pass it to the ```sliding_event``` function the landing points look closer to what we expect.\n",
    "\n",
    "To illustrate, we first generate different starting points for the 4 PC channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b1a19-dfa2-40f4-9332-591fe4c36018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:59.151645Z",
     "iopub.status.busy": "2023-10-16T10:49:59.151323Z",
     "iopub.status.idle": "2023-10-16T10:49:59.156798Z",
     "shell.execute_reply": "2023-10-16T10:49:59.156349Z",
     "shell.execute_reply.started": "2023-10-16T10:49:59.151624Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "magnitudes_starting_points = init.gen_mags(n_events=1, decimate=3)#decimate reduces the possible number of starting points\n",
    "magnitudes_starting_points[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4229e8f-8c28-4659-9d83-4d5bedfb99a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:59.157756Z",
     "iopub.status.busy": "2023-10-16T10:49:59.157432Z",
     "iopub.status.idle": "2023-10-16T10:49:59.529121Z",
     "shell.execute_reply": "2023-10-16T10:49:59.528629Z",
     "shell.execute_reply.started": "2023-10-16T10:49:59.157739Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init.sliding_event(magnitudes=magnitudes_starting_points, step=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1214c6-b0f1-4714-a5e4-5c1ee479c31d",
   "metadata": {},
   "source": [
    "It appears that several landings points appear with some redundancies. To group those points together we use a clustering algorithm and make a choice on the number of clusters based on the grouping quality (silhouette coefficients and visual inspection).\n",
    "\n",
    "First we automatically apply the method above by calling `sliding_event_mags`. Next, we cluster the events using kmeans clustering. Based on the silhoutte coefficients the method proposes an optimal number of clusters. If you agree you should enter 0, if not you can propose another number of clusters. Finally, we fit and display the model you preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8cc64-165c-474b-ad41-d11126e2cbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:49:59.530398Z",
     "iopub.status.busy": "2023-10-16T10:49:59.530077Z",
     "iopub.status.idle": "2023-10-16T10:50:22.828192Z",
     "shell.execute_reply": "2023-10-16T10:50:22.827855Z",
     "shell.execute_reply.started": "2023-10-16T10:49:59.530376Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calc estimates\n",
    "lkhs, mags, channels, times = init.sliding_event_mags(epoch_data, step=1, decimate_grid = 3, cpu=cpus, plot=True, min_iteration=10)\n",
    "\n",
    "# cluster in time, lkh, and mags\n",
    "mags_cl, pars_cl = hmp.clusters.cluster_events(init, lkhs, mags, channels, times, method='time_x_lkh_x_mags', max_clust=10, p_outlier=.05, info=info, calc_outliers=True)\n",
    "\n",
    "#fit final model\n",
    "best_estimate = init.fit_single(mags_cl.shape[0], magnitudes=mags_cl,parameters=pars_cl)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, best_estimate, info, init, magnify=1, sensors=False, time_step=1000/init.sfreq,xlabel='Time (ms)', contours=0, event_lines=True, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60cfff-2346-43ac-88e7-5ce55d7863b9",
   "metadata": {},
   "source": [
    "The solution isn't yet perfect and can take quite some time depending on the number of principal components (and therefore combinations of starting points) and the number of tested clusters. One possible explanation is that not taking into account the previous events makes it harder to detect the next one (that is probably why we are missing the last one here). Therefore we created a method combining all the properties of this way of exploring the starting points:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77643eee-ec31-4da7-bd53-cc3c33d51a36",
   "metadata": {},
   "source": [
    "## Cumulative event fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976431f-c769-4cf9-9d71-2726f8312284",
   "metadata": {},
   "source": [
    "Instead of fitting an _n_ event model, as the ```sliding_event()``` function, the new method starts by fitting a 1 event model (two stages) using each sample from the time 0 (stimulus onset) to the mean RT. Therefore it tests for the landing point of the expectation maximization algorithm given each sample as starting point and the likelihood associated with this landing point. As soon as a starting points reaches the convergence criterion, the function fits an _n+1_ event model and uses the next samples in the RT for the following event, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fafcbc-0b53-4e71-9082-5c43c11cb818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:50:22.828793Z",
     "iopub.status.busy": "2023-10-16T10:50:22.828678Z",
     "iopub.status.idle": "2023-10-16T10:50:23.724337Z",
     "shell.execute_reply": "2023-10-16T10:50:23.723719Z",
     "shell.execute_reply.started": "2023-10-16T10:50:22.828784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimates = init.fit(diagnostic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ec398-371f-4b6a-98b8-d318add0aa08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:50:47.954780Z",
     "iopub.status.busy": "2023-10-16T10:50:47.954593Z",
     "iopub.status.idle": "2023-10-16T10:50:48.376948Z",
     "shell.execute_reply": "2023-10-16T10:50:48.376624Z",
     "shell.execute_reply.started": "2023-10-16T10:50:47.954766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates, info, init, \n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66126dc9-4489-4c37-9db5-f3aa2662c4ab",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "What can cause the failure to capture the three successive events knowing that their topologies are quite similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5023495-1396-459b-aa42-188cbc301bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:50:54.465488Z",
     "iopub.status.busy": "2023-10-16T10:50:54.465314Z",
     "iopub.status.idle": "2023-10-16T10:50:55.182235Z",
     "shell.execute_reply": "2023-10-16T10:50:55.181815Z",
     "shell.execute_reply.started": "2023-10-16T10:50:54.465477Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected = bests.sel(n_events=n_events)\n",
    "hmp.visu.plot_topo_timecourse(epoch_data, selected, info, init, sensors=False,\n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952cd799",
   "metadata": {},
   "source": [
    "If you thought of the problem, you can fix it using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bdf1f-27bb-423d-a497-515ae6896a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:50:55.946115Z",
     "iopub.status.busy": "2023-10-16T10:50:55.945969Z",
     "iopub.status.idle": "2023-10-16T10:50:58.772899Z",
     "shell.execute_reply": "2023-10-16T10:50:58.772349Z",
     "shell.execute_reply.started": "2023-10-16T10:50:55.946104Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp_dat = hmp.utils.transform_data(epoch_data, apply_standard=False)\n",
    "#Plot new PC selection\n",
    "hmp.visu.plot_components_sensor(hmp_dat, positions=info)\n",
    "#Re-Initialization of the model\n",
    "init = hmp.models.hmp(hmp_dat, sfreq=epoch_data.sfreq, event_width=50, cpus=cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff5ec5-291a-4c13-9b82-4249722ad584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T08:59:48.187183Z",
     "iopub.status.busy": "2023-07-18T08:59:48.186676Z",
     "iopub.status.idle": "2023-07-18T08:59:48.322229Z",
     "shell.execute_reply": "2023-07-18T08:59:48.320967Z",
     "shell.execute_reply.started": "2023-07-18T08:59:48.187146Z"
    },
    "tags": []
   },
   "source": [
    "Now, re-run the fit, and see how may events it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634349e-9652-4444-b89a-795cd37f3516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:51:00.489162Z",
     "iopub.status.busy": "2023-10-16T10:51:00.488996Z",
     "iopub.status.idle": "2023-10-16T10:51:01.978178Z",
     "shell.execute_reply": "2023-10-16T10:51:01.977738Z",
     "shell.execute_reply.started": "2023-10-16T10:51:00.489151Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimates = init.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c59f7-7562-4f77-8944-ee04f314eec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:51:01.978907Z",
     "iopub.status.busy": "2023-10-16T10:51:01.978803Z",
     "iopub.status.idle": "2023-10-16T10:51:02.642358Z",
     "shell.execute_reply": "2023-10-16T10:51:02.641976Z",
     "shell.execute_reply.started": "2023-10-16T10:51:01.978898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.visu.plot_topo_timecourse(epoch_data, estimates, info, init, \n",
    "                                times_to_display = np.mean(np.cumsum(sim_source_times,axis=1),axis=0), magnify=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dcb180-cda4-4cd5-b30e-8aaf66580bc7",
   "metadata": {},
   "source": [
    "# Data saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd030f8-461a-433d-9f31-ee440e4be43b",
   "metadata": {},
   "source": [
    "Once finished we can save fitted models using the dedicated command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0c3f1-062b-40f1-a49f-af6f3845895a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:51:05.226387Z",
     "iopub.status.busy": "2023-10-16T10:51:05.225972Z",
     "iopub.status.idle": "2023-10-16T10:51:05.249466Z",
     "shell.execute_reply": "2023-10-16T10:51:05.248982Z",
     "shell.execute_reply.started": "2023-10-16T10:51:05.226355Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.utils.save_fit(selected, 'selected.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87214cd-3795-4827-b6c9-249d5e6cf27f",
   "metadata": {},
   "source": [
    "And load the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d5e99-cd2f-4553-9ca3-1c56e01a04c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:51:06.515452Z",
     "iopub.status.busy": "2023-10-16T10:51:06.514984Z",
     "iopub.status.idle": "2023-10-16T10:51:06.537870Z",
     "shell.execute_reply": "2023-10-16T10:51:06.537471Z",
     "shell.execute_reply.started": "2023-10-16T10:51:06.515411Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimates = hmp.utils.load_fit('selected.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afd3e4-8467-4e25-963a-5de8f44447a6",
   "metadata": {},
   "source": [
    "Or even only save the estimated event probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c3b24-d271-4bd3-a7a8-fe552c087b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:51:07.115006Z",
     "iopub.status.busy": "2023-10-16T10:51:07.114772Z",
     "iopub.status.idle": "2023-10-16T10:51:07.358865Z",
     "shell.execute_reply": "2023-10-16T10:51:07.358407Z",
     "shell.execute_reply.started": "2023-10-16T10:51:07.114991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmp.utils.save_eventprobs(selected.eventprobs, 'selected_eventprobs.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
